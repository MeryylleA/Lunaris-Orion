# Lunar Core ğŸŒ™

Um modelo VAE (Variational Autoencoder) especializado em geraÃ§Ã£o e reconstruÃ§Ã£o de pixel art 16x16, implementado em JAX e Flax.

## ğŸ—ï¸ Arquitetura

<p align="center">
  <img src="docs/images/lunar_core_architecture.svg" alt="Arquitetura do Lunar Core" width="500">
</p>

<details>
<summary><strong>Estrutura Detalhada do Modelo</strong></summary>

```
Input (16Ã—16Ã—3) â†’ Encoder
  â”œâ”€ Conv2D (64) + BN + ReLU
  â”œâ”€ 3Ã— ResBlock (64)
  â”œâ”€ DownConv (128)
  â”œâ”€ 3Ã— ResBlock (128)
  â”œâ”€ DownConv (256)
  â”œâ”€ 3Ã— ResBlock (256)
  â””â”€ Dense â†’ Latent (256)
      â†“
Latent Space (256)
      â†“
Dense â†’ Reshape â†’ Decoder
  â”œâ”€ 3Ã— ResBlock (256)
  â”œâ”€ UpConv (128)
  â”œâ”€ 3Ã— ResBlock (128)
  â”œâ”€ UpConv (64)
  â”œâ”€ 3Ã— ResBlock (64)
  â”œâ”€ Conv2D (3) + Tanh
  â””â”€ Output (16Ã—16Ã—3)
```
</details>

### Componentes Principais:
- **Encoder**: Comprime a imagem em um espaÃ§o latente de 256 dimensÃµes
- **Decoder**: ReconstrÃ³i a imagem a partir do espaÃ§o latente
- **Blocos Residuais**: 3 blocos por nÃ­vel para melhor aprendizado
- **Skip Connections**: ConexÃµes residuais preservam detalhes

## ğŸŒŸ CaracterÃ­sticas

- **Arquitetura Especializada**:
  - VAE com blocos residuais otimizados para pixel art
  - DimensÃ£o latente de 256
  - Encoder-Decoder simÃ©trico com 3 nÃ­veis de resoluÃ§Ã£o
  - Blocos residuais com normalizaÃ§Ã£o em batch

- **Performance**:
  - Treinamento rÃ¡pido (~173s/epoch em GPU)
  - Suporte a mixed precision (FP16/FP32)
  - Otimizado para GPUs NVIDIA
  - CompatÃ­vel com CPU

- **Recursos AvanÃ§ados**:
  - Transfer Learning integrado
  - Data augmentation especÃ­fico para pixel art
  - Logging com Weights & Biases
  - Checkpoints automÃ¡ticos

## ğŸ”§ Arquitetura

### Encoder
```
Input (16x16x3)
   â†“
Conv (3x3, 64 filtros) + BatchNorm + ReLU
   â†“
3x ResBlocks (64 filtros)
   â†“
Downsample + Conv (128 filtros)
   â†“
3x ResBlocks (128 filtros)
   â†“
Downsample + Conv (256 filtros)
   â†“
3x ResBlocks (256 filtros)
   â†“
Dense â†’ Latent Space (256 dim)
```

### Decoder
```
Latent Space (256 dim)
   â†“
Dense â†’ Reshape
   â†“
3x ResBlocks (256 filtros)
   â†“
Upsample + Conv (128 filtros)
   â†“
3x ResBlocks (128 filtros)
   â†“
Upsample + Conv (64 filtros)
   â†“
3x ResBlocks (64 filtros)
   â†“
Conv (3x3, 3 filtros) + Tanh
   â†“
Output (16x16x3)
```

## ğŸ“Š Resultados

- **MÃ©tricas de Treinamento**:
  - Loss inicial: ~0.3864
  - Loss final: ~0.0943
  - Validation Loss: ~0.0849
  - Tempo mÃ©dio por Ã©poca: 173s

## ğŸš€ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/seu-usuario/lunar-core.git
cd lunar-core
```

2. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

## ğŸ’» Uso

### Treinamento

1. Configure os parÃ¢metros em `config/config.yaml`
2. Execute o treinamento:
```bash
python training/train.py
```

### InferÃªncia

Para gerar novas pixel arts:
```bash
python inference/inference.py
```

## âš™ï¸ ConfiguraÃ§Ã£o

O arquivo `config.yaml` permite configurar:

```yaml
model:
  latent_dim: 256
  filters: [64, 128, 256]
  num_residual_blocks: 3
  
training:
  batch_size: 64
  learning_rate: 0.0002
  num_epochs: 100
  
hardware:
  mixed_precision: true
  device_priority: ["gpu", "cpu"]
```

## ğŸ“ CitaÃ§Ã£o

Se vocÃª usar este modelo em sua pesquisa, por favor cite:

```bibtex
@software{lunar_core2024,
  title={Lunar Core: A Specialized VAE for Pixel Art Generation},
  author={Seu Nome},
  year={2024},
  publisher={GitHub},
  url={https://github.com/seu-usuario/lunar-core}
}
```

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. Fork o projeto
2. Crie sua Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a Branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## âœ¨ Agradecimentos

- JAX Team pela excelente biblioteca
- Comunidade Flax pelos recursos
- Todos os contribuidores 