# Arquitetura LunarisCore - Documentação Completa

## 1. Visão Geral

O LunarisCore é um modelo de geração de pixel art baseado em uma arquitetura VAE (Variational Autoencoder) com suporte a condicionamento por texto. A arquitetura foi projetada para ser flexível, permitindo treinamento em diferentes modos e com diferentes configurações.

## 2. Componentes Principais

### 2.1 Encoder
```python
class Encoder(nn.Module):
    """
    Encoder do VAE que processa imagens de pixel art.
    - Input: Imagens [B, H, W, C]
    - Output: (mean, logvar) do espaço latente
    """
    latent_dim: int
    filters: Sequence[int]
    
    def setup(self):
        # Camadas convolucionais com downsampling
        self.conv_layers = []
        for f in self.filters:
            self.conv_layers.extend([
                nn.Conv(f, kernel_size=(3, 3), strides=(2, 2)),
                nn.BatchNorm(),
                nn.relu
            ])
            
        # Projeção para o espaço latente
        self.mean = nn.Dense(self.latent_dim)
        self.logvar = nn.Dense(self.latent_dim)
```

### 2.2 Decoder
```python
class Decoder(nn.Module):
    """
    Decoder do VAE que reconstrói imagens de pixel art.
    - Input: Vetor latente [B, latent_dim]
    - Output: Imagem reconstruída [B, H, W, C]
    """
    output_shape: Sequence[int]
    filters: Sequence[int]
    
    def setup(self):
        # Projeção inicial
        self.dense = nn.Dense(self.filters[0] * 4 * 4)
        
        # Camadas convolucionais com upsampling
        self.conv_layers = []
        for f in self.filters[1:]:
            self.conv_layers.extend([
                nn.ConvTranspose(f, kernel_size=(3, 3), strides=(2, 2)),
                nn.BatchNorm(),
                nn.relu
            ])
            
        # Camada final para reconstrução
        self.final = nn.Conv(self.output_shape[-1], kernel_size=(3, 3))
```

### 2.3 TextEncoder
```python
class TextEncoder(nn.Module):
    """
    Encoder de texto baseado em transformer.
    - Input: Tokens de texto [B, seq_len]
    - Output: Embeddings [B, output_dim]
    """
    output_dim: int
    hidden_dim: int = 512
    intermediate_dim: int = 2048
    num_layers: int = 3
    num_heads: int = 8
    dropout_rate: float = 0.1
    
    def setup(self):
        # Embedding de tokens
        self.token_embedding = nn.Embed(
            num_embeddings=50257,  # Vocabulário GPT-2
            features=self.hidden_dim
        )
        
        # Camadas do transformer
        self.transformer_blocks = [
            TransformerBlock(
                hidden_dim=self.hidden_dim,
                intermediate_dim=self.intermediate_dim,
                num_heads=self.num_heads,
                dropout_rate=self.dropout_rate
            ) for _ in range(self.num_layers)
        ]
        
        # Projeção final
        self.output_projection = nn.Dense(self.output_dim)
```

### 2.4 LunarCore (Modelo Principal)
```python
class LunarCore(nn.Module):
    """
    Modelo principal que integra todos os componentes.
    Suporta dois modos de treinamento:
    1. Pixel Art: Apenas reconstrução de imagens
    2. Prompt: Geração condicionada por texto
    """
    latent_dim: int
    filters: Sequence[int]
    input_shape: Sequence[int]
    use_text: bool = False
    fusion_type: str = 'concat'
    
    def setup(self):
        # Componentes principais
        self.encoder = Encoder(self.latent_dim, self.filters)
        self.decoder = Decoder(self.input_shape, self.filters[::-1])
        
        if self.use_text:
            self.text_encoder = TextEncoder(
                output_dim=self.latent_dim,
                hidden_dim=512,
                num_layers=3
            )
            
            # Módulo de fusão texto-imagem
            if self.fusion_type == 'concat':
                self.fusion = LatentConcat()
            elif self.fusion_type == 'add':
                self.fusion = LatentAdd()
            else:
                self.fusion = LatentGating()
```

## 3. Fluxo de Dados

### 3.1 Modo Pixel Art
1. Imagem de entrada -> Encoder -> (mean, logvar)
2. Reparametrização -> z = mean + exp(0.5 * logvar) * epsilon
3. z -> Decoder -> Imagem reconstruída
4. Loss = Reconstrução (MSE) + KL Divergence

### 3.2 Modo Prompt
1. Imagem -> Encoder -> (mean_img, logvar_img)
2. Texto -> TextEncoder -> text_embedding
3. Fusão: (z_img, text_embedding) -> z_combined
4. z_combined -> Decoder -> Imagem reconstruída
5. Loss = Reconstrução + KL + Alinhamento texto-imagem

## 4. Configurações e Hiperparâmetros

```yaml
model:
  latent_dim: 256
  filters: [64, 128, 256, 512]
  input_shape: [16, 16, 3]
  use_text: true
  text_encoder_config:
    hidden_dim: 512
    intermediate_dim: 2048
    num_layers: 3
    num_heads: 8
    dropout_rate: 0.1
  latent_fusion:
    type: 'concat'  # ou 'add', 'gate'

training:
  batch_size: 64
  learning_rate: 0.0002
  beta1: 0.5
  beta2: 0.999
  num_epochs: 100
  save_interval: 5
  eval_interval: 1
```

## 5. Características Especiais

1. **Arquitetura Modular**: Cada componente é independente e pode ser ajustado separadamente.
2. **Flexibilidade de Treinamento**: Suporta treinamento progressivo (primeiro pixel art, depois prompts).
3. **Múltiplos Tipos de Fusão**: Diferentes estratégias para combinar informações de texto e imagem.
4. **Normalização Adaptativa**: Uso de BatchNorm para treinamento estável.
5. **Pipeline de Dados Otimizado**: Conversão eficiente entre TensorFlow e JAX.

## 6. Uso do Modelo

### 6.1 Geração de Pixel Art
```python
# Geração simples
image = model.generate(batch_size=1)

# Geração com prompt
image = model.generate(prompt="A cute pixel art cat", batch_size=1)
```

### 6.2 Interpolação no Espaço Latente
```python
# Entre duas imagens
interpolated = model.interpolate(image1, image2, steps=10)

# Entre dois prompts
interpolated = model.interpolate_prompts(
    "A pixel art castle",
    "A pixel art dragon",
    steps=10
)
```

## 7. Recomendações de Uso

1. **Treinamento**: Comece com o modo pixel art e depois fine-tune com prompts.
2. **Dados**: Use imagens de pixel art de alta qualidade e bem alinhadas.
3. **Prompts**: Prefira descrições claras e específicas.
4. **Otimização**: Ajuste learning rate e batch size conforme necessário.
5. **Validação**: Monitore FID score e qualidade visual das amostras.

## 8. Limitações Conhecidas

1. Resolução limitada (16x16 pixels)
2. Pode ter dificuldade com detalhes muito finos
3. Geração condicional depende da qualidade dos prompts
4. Requer GPU para treinamento eficiente

## 9. Futuras Melhorias Planejadas

1. Suporte a resoluções maiores
2. Melhor controle de estilo
3. Integração com outros modelos de linguagem
4. Otimização de performance
5. Suporte a geração em lote maior

## 10. Contribuições e Agradecimentos

Desenvolvido como parte do projeto Lunaris Orion, com contribuições da comunidade.
Agradecimentos especiais aos desenvolvedores e artistas que forneceram dados de treinamento. 